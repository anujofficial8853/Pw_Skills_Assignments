{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble Learning | Assignment"
      ],
      "metadata": {
        "id": "zf_HSxBmBcPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-1 : What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it."
      ],
      "metadata": {
        "id": "j5MLmHUNBiaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A-1 : What is Ensemble Learning in Machine Learning?\n",
        "Ensemble Learning is a technique in machine learning where multiple models (learners) are combined to solve a problem and improve overall performance. Instead of relying on a single model, ensemble methods merge predictions from several models to produce a more accurate and robust outcome.\n",
        "\n",
        "Key Idea Behind Ensemble Learning:\n",
        "The core idea is that a group of weak learners (models that perform slightly better than random guessing) can come together to form a strong learner. By combining diverse models, ensemble learning helps to:\n",
        "\n",
        "Reduce variance (e.g., Bagging methods like Random Forest)\n",
        "\n",
        "Reduce bias (e.g., Boosting methods like AdaBoost, Gradient Boosting)\n",
        "\n",
        "Improve accuracy and generalization\n",
        "\n",
        "Common Ensemble Methods:\n",
        "Bagging: Builds multiple models on different random subsets of data (e.g., Random Forest).\n",
        "\n",
        "Boosting: Sequentially builds models, focusing on correcting previous errors (e.g., AdaBoost, XGBoost).\n",
        "\n",
        "Voting: Combines predictions from multiple models using majority or average vote."
      ],
      "metadata": {
        "id": "-wf7Zi8RBoBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-2 : What is the difference between Bagging and Boosting?\n"
      ],
      "metadata": {
        "id": "gfb6EpwdByrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A-2 : What is the difference between Bagging and Boosting?\n",
        "Bagging (Bootstrap Aggregating) and Boosting are both ensemble learning techniques that combine multiple models to improve prediction performance, but they differ in how models are trained and combined.\n",
        "\n",
        "Bagging:\n",
        "Trains multiple models independently on random subsets of the training data (with replacement).\n",
        "\n",
        "Goal: Reduce variance by averaging or voting.\n",
        "\n",
        "All models have equal weight in the final prediction.\n",
        "\n",
        "Example: Random Forest.\n",
        "\n",
        "Boosting:\n",
        "Trains models sequentially, where each new model focuses on correcting the errors of the previous ones.\n",
        "\n",
        "Goal: Reduce bias and build a strong learner from weak ones.\n",
        "\n",
        "Later models have more influence in the final result.\n",
        "\n",
        "Example: AdaBoost, Gradient Boosting.\n",
        "\n",
        "Key Differences Summary:\n",
        "Bagging = Parallel, Boosting = Sequential\n",
        "\n",
        "Bagging reduces variance, Boosting reduces bias\n",
        "\n",
        "Bagging treats all models equally, Boosting gives weights based on performance"
      ],
      "metadata": {
        "id": "VBzrHBNtCCck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-3 : : What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?"
      ],
      "metadata": {
        "id": "3VduTueQCLuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A-3 : What is Bootstrap Sampling and its Role in Bagging Methods like Random Forest?\n",
        "Bootstrap sampling is a statistical technique that involves randomly selecting samples from a dataset with replacement. This means the same data point can appear multiple times in a single sample.\n",
        "\n",
        "In the context of Bagging (Bootstrap Aggregating) methods like Random Forest, bootstrap sampling plays a key role in creating diverse training subsets.\n",
        "\n",
        "Role in Bagging / Random Forest:\n",
        "Diversity: Each model (e.g., decision tree) is trained on a different random bootstrap sample, increasing model diversity.\n",
        "\n",
        "Overfitting Reduction: Combining predictions from multiple diverse models helps to reduce overfitting and improve generalization.\n",
        "\n",
        "Stability: Models trained on slightly different datasets help in stabilizing predictions, especially in noisy datasets.\n",
        "\n"
      ],
      "metadata": {
        "id": "UNj43xGZCQZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-4 : What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?"
      ],
      "metadata": {
        "id": "U8wjYUIyCakh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A-4 : What are Out-of-Bag (OOB) Samples and the OOB Score?\n",
        "In Bagging methods like Random Forest, each model (e.g., decision tree) is trained on a bootstrap sample — a random subset of the training data with replacement.\n",
        "\n",
        "As a result, about 63% of the data points are included in each bootstrap sample, and the remaining ~37% are not selected. These unused data points are called Out-of-Bag (OOB) samples.\n",
        "\n",
        "How OOB Samples Are Used:\n",
        "OOB samples serve as a kind of internal validation set.\n",
        "\n",
        "Each data point is predicted using only the models that did not see it during training.\n",
        "\n",
        "The model’s predictions on OOB samples are compared to their actual labels to calculate the OOB score.\n",
        "\n",
        "OOB Score (Evaluation):\n",
        "The OOB score is the average accuracy (or other metric) of the model on the OOB samples.\n",
        "\n",
        "It provides a reliable estimate of model performance without needing a separate validation set.\n",
        "\n",
        "It helps save data and gives a quick, unbiased estimate of generalization error.\n",
        "\n"
      ],
      "metadata": {
        "id": "nZB0WGVvCgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-5 : Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest"
      ],
      "metadata": {
        "id": "Hqz8jMyICl3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A-5 :  1. In a Single Decision Tree:\n",
        "Feature importance is calculated based on how much each feature reduces impurity (e.g., Gini or Entropy) when used for splitting.\n",
        "\n",
        "It reflects the importance of features in that one specific tree.\n",
        "\n",
        "Can be unstable — small changes in data may lead to very different trees and importance rankings.\n",
        "\n",
        "Prone to overfitting if the tree is deep or not pruned.\n",
        "\n",
        "2. In a Random Forest:\n",
        "Feature importance is averaged across all trees in the ensemble.\n",
        "\n",
        "More stable and reliable, as it reduces the bias of any single tree.\n",
        "\n",
        "Random Forest accounts for feature interactions and variability, giving a more generalized measure.\n",
        "\n",
        "Often gives better insight into which features consistently contribute to model performance."
      ],
      "metadata": {
        "id": "YgiSNi5yCs7e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-6 : Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "De_k4kajC2tN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Create a DataFrame for feature importance\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort and display the top 5 important features\n",
        "top_features = feature_importance_df.sort_values(by='Importance', ascending=False).head(5)\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(top_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jokA0RoC-4W",
        "outputId": "22a2db3c-62b7-4849-da0e-48fa71181c7e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-7 :  Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "exkhvS4PDNbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree\n",
        "dtree = DecisionTreeClassifier(random_state=42)\n",
        "dtree.fit(X_train, y_train)\n",
        "y_pred_tree = dtree.predict(X_test)\n",
        "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
        "\n",
        "# Train a Bagging Classifier using Decision Trees\n",
        "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "# Print results\n",
        "print(f\"Single Decision Tree Accuracy: {accuracy_tree:.2f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy_bagging:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lipwFnZFDU7r",
        "outputId": "b915024a-f2ce-4b1b-f360-7dea2a9eef39"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Decision Tree Accuracy: 1.00\n",
            "Bagging Classifier Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-8 :  Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "zvCXBhIMDgtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 50, 100],\n",
        "    'max_depth': [None, 3, 5, 10]\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(rf, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best model and evaluate\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Final Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsbXsmjQDmjB",
        "outputId": "62b50c89-7a84-4883-a6da-e3abebca7203"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 10}\n",
            "Final Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-9 : Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "K02C_cvYDr9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Regressor with Decision Trees\n",
        "bagging_model = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_model.predict(X_test)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Errors\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Print Results\n",
        "print(f\"Bagging Regressor MSE: {mse_bagging:.2f}\")\n",
        "print(f\"Random Forest Regressor MSE: {mse_rf:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hkjIn9IDwc1",
        "outputId": "0e688459-6879-4d18-a94e-9d99dacca234"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.26\n",
            "Random Forest Regressor MSE: 0.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-10 : You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context."
      ],
      "metadata": {
        "id": "3Pr83pkkECGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A-10 :1. Choose Between Bagging or Boosting\n",
        "We'll use Boosting (XGBoost) because:\n",
        "\n",
        "It works well with tabular and imbalanced data.\n",
        "\n",
        "Learns sequentially, focusing on difficult cases.\n",
        "\n",
        "Usually gives better performance for structured datasets like financial records.\n",
        "\n",
        "2. Handle Overfitting\n",
        "We’ll control overfitting using:\n",
        "\n",
        "Regularization (max_depth, gamma, min_child_weight)\n",
        "\n",
        "Early stopping\n",
        "\n",
        "Cross-validation\n",
        "\n",
        "3. Base Model: XGBoost (Decision Trees internally)\n",
        "4. Evaluation: Stratified K-Fold Cross-Validation with AUC, Precision, Recall\n"
      ],
      "metadata": {
        "id": "trIGgT__EWgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Simulate an imbalanced loan default dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=6, n_redundant=2,\n",
        "                           n_classes=2, weights=[0.85, 0.15], flip_y=0.01, random_state=42)\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Define the XGBoost classifier with regularization\n",
        "model = XGBClassifier(\n",
        "    max_depth=4,\n",
        "    learning_rate=0.1,\n",
        "    n_estimators=100,\n",
        "    scale_pos_weight=5,  # To handle imbalance\n",
        "    eval_metric='auc',\n",
        "    use_label_encoder=False,\n",
        "    verbosity=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(model, X_train, y_train, scoring='roc_auc', cv=skf)\n",
        "\n",
        "print(\"Cross-Validation AUC Scores:\", np.round(cv_scores, 3))\n",
        "print(\"Mean AUC Score: {:.3f}\".format(np.mean(cv_scores)))\n",
        "\n",
        "# Train on the entire training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Classification metrics\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Test ROC-AUC Score: {:.3f}\".format(roc_auc_score(y_test, y_prob)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5--I94qYEhtg",
        "outputId": "5ada4280-81be-48dc-e584-4a8db3be9070"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation AUC Scores: [0.932 0.944 0.953 0.956 0.954]\n",
            "Mean AUC Score: 0.948\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.98      0.97       169\n",
            "           1       0.86      0.77      0.81        31\n",
            "\n",
            "    accuracy                           0.94       200\n",
            "   macro avg       0.91      0.88      0.89       200\n",
            "weighted avg       0.94      0.94      0.94       200\n",
            "\n",
            "Test ROC-AUC Score: 0.947\n"
          ]
        }
      ]
    }
  ]
}