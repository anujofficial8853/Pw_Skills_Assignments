{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#SVM & Naive Bayes : Assignment"
      ],
      "metadata": {
        "id": "g_N_v03N9Y7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-1 : What is a Support Vector Machine (SVM), and how does it work?\n"
      ],
      "metadata": {
        "id": "bGcsxCfT9nuP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A-1 : Support Vector Machine (SVM) –\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression, especially binary classification.\n",
        "\n",
        "SVM works by finding the optimal hyperplane that best separates the data into classes with the maximum margin. The data points closest to the hyperplane are called support vectors — they define the margin.\n",
        "\n",
        "If data is not linearly separable, SVM uses kernel functions (like RBF or polynomial) to map data into higher dimensions where separation is possible. It also supports a soft margin for handling noisy data.\n",
        "\n",
        "SVM is effective for high-dimensional data and is widely used in text classification, face detection, and bioinformatics.\n",
        "\n"
      ],
      "metadata": {
        "id": "FztQhCAi9uyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-2 : Explain the difference between Hard Margin and Soft Margin SVM."
      ],
      "metadata": {
        "id": "Icp4lCku-Kkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A-2 : In Hard Margin SVM, the algorithm assumes that the data is perfectly linearly separable, meaning no data points fall inside the margin or on the wrong side of the hyperplane. It aims to find the hyperplane with the maximum margin that strictly separates the two classes without allowing any errors. However, this approach is very sensitive to noise and can lead to overfitting if the data isn't perfectly clean.\n",
        "\n",
        "On the other hand, Soft Margin SVM allows the model to tolerate some misclassifications or violations of the margin to achieve better performance on noisy or non-linearly separable data. It introduces a trade-off between maximizing the margin and minimizing classification errors. This makes Soft Margin SVM more flexible and practical for real-world applications where data is often messy or overlapping.\n",
        "\n"
      ],
      "metadata": {
        "id": "mit3tPgC-Vco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-3 : What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n"
      ],
      "metadata": {
        "id": "snpKMv9G-mpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A-3 : The Kernel Trick in SVM is a technique used to solve problems where data is not linearly separable. Instead of explicitly mapping data to a higher-dimensional space, the kernel trick uses a kernel function to compute the dot product of the transformed data directly in the original space. This makes computations efficient while allowing SVM to learn non-linear decision boundaries.\n",
        "\n",
        "One commonly used kernel is the Radial Basis Function (RBF) kernel, also known as the Gaussian kernel. It transforms data into an infinite-dimensional space and is useful when the data has complex patterns.\n",
        "\n",
        "Use Case: The RBF kernel is widely used in image classification, handwriting recognition, and medical diagnosis, where the data cannot be separated by a straight line."
      ],
      "metadata": {
        "id": "82SG2u6--tRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-4 :  What is a Naïve Bayes Classifier, and why is it called “naïve”?"
      ],
      "metadata": {
        "id": "ylnN7kr--1DQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A-4 : A Naïve Bayes Classifier is a supervised machine learning algorithm based on Bayes' Theorem, used mainly for classification tasks. It calculates the probability that a given data point belongs to a particular class, based on the values of its features.\n",
        "\n",
        "The classifier is called “naïve” because it assumes that all features are independent of each other given the class label — an assumption that is rarely true in real-world data. Despite this unrealistic assumption, the algorithm performs surprisingly well in many applications, especially with large datasets.\n",
        "\n",
        "Naïve Bayes is commonly used in spam detection, sentiment analysis, and document classification, due to its simplicity, speed, and effectiveness."
      ],
      "metadata": {
        "id": "yCEVaQ8M_T3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-5 : Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?"
      ],
      "metadata": {
        "id": "IOgaCbcu_WWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A-5 : Gaussian, Multinomial, and Bernoulli Naïve Bayes – Description and Use Cases\n",
        "Gaussian Naïve Bayes:\n",
        "This variant assumes that the features follow a normal (Gaussian) distribution. It is best suited for continuous numerical data.\n",
        "Use Case: Useful in medical diagnosis or iris flower classification, where features are continuous (e.g., height, weight).\n",
        "\n",
        "Multinomial Naïve Bayes:\n",
        "It is designed for discrete data, particularly for count-based features like word frequencies.\n",
        "Use Case: Ideal for text classification, such as spam detection or document categorization.\n",
        "\n",
        "Bernoulli Naïve Bayes:\n",
        "This variant works with binary/boolean features (0 or 1), representing the presence or absence of a feature.\n",
        "Use Case: Suitable for tasks like email classification, where features are binary (e.g., presence of specific words)."
      ],
      "metadata": {
        "id": "7SVIHX4x_b-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-6 : Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "Mpk6hAW5_nlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an SVM classifier with a linear kernel\n",
        "clf = SVC(kernel='linear')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print support vectors\n",
        "print(\"Support Vectors:\")\n",
        "print(clf.support_vectors_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cqgP39U_yIP",
        "outputId": "e8ceed0d-9c55-438a-e71c-e789447119b4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "Support Vectors:\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-7: Write a Python program to:\n",
        "Load the Breast Cancer dataset\n",
        " Train a Gaussian Naïve Bayes model\n",
        " Print its classification report including precision, recall, and F1-score.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "P2lmKkHZ_4_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1hV-nYNAL0T",
        "outputId": "2f4accc9-156c-47e1-bae4-15fd13dd746b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-8 : Write a Python program to:\n",
        "Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "Print the best hyperparameters and accuracy.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "dO0LfiLpAKK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# Create SVM classifier\n",
        "svc = SVC()\n",
        "\n",
        "# Perform Grid Search with 5-fold cross-validation\n",
        "grid = GridSearchCV(svc, param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = grid.predict(X_test)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(\"Best Hyperparameters:\", grid.best_params_)\n",
        "print(\"Test Accuracy: {:.2f}\".format(accuracy_score(y_test, y_pred)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U2NmWV7Aasa",
        "outputId": "a899f680-e110-408c-8331-6647393fc156"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Test Accuracy: 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-9 : Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predictions.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "X3pU5z3wAKRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Load binary text classification dataset (2 categories)\n",
        "categories = ['rec.sport.hockey', 'sci.space']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(newsgroups.data)\n",
        "y = newsgroups.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Naive Bayes classifier\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for ROC-AUC\n",
        "y_probs = nb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate and print ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opMBB_S-Aq7o",
        "outputId": "8d0f1b77-39f3-4da0-b19b-53c0a69ae7f8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-10 : Question 10: Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "dPe-buVqA1wx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "\n",
        "# Simulate a spam vs. ham dataset using two distinct categories\n",
        "categories = ['talk.politics.misc', 'rec.autos']  # simulate 'ham' and 'spam'\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Add missing data simulation\n",
        "texts = np.array(data.data, dtype=object)\n",
        "texts[5] = None  # Inject a missing email\n",
        "\n",
        "# Handle missing data by replacing None with an empty string before vectorization\n",
        "texts = np.array([text if text is not None else '' for text in texts])\n",
        "\n",
        "# Vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(texts)\n",
        "y = data.target  # 0 = ham, 1 = spam (simulated)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluation\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC Score: {:.2f}\".format(roc_auc_score(y_test, y_prob)))\n",
        "\n",
        "# Business Impact explanation (as requested in the Markdown cell)\n",
        "print(\"\\nBusiness Impact:\")\n",
        "print(\"Implementing this email classification system can significantly improve efficiency by automatically filtering spam, reducing the time employees spend sorting emails.\")\n",
        "print(\"It can also enhance security by minimizing exposure to malicious content often found in spam.\")\n",
        "print(\"Accurate classification leads to a better user experience and ensures important communications are not missed.\")\n",
        "print(\"The chosen metrics (precision, recall, F1-score, ROC-AUC) are relevant for evaluating a spam filter. High precision minimizes legitimate emails being marked as spam, while high recall minimizes spam emails reaching the inbox. ROC-AUC provides an overall measure of the model's ability to distinguish between classes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoNLjOOqA4qe",
        "outputId": "66d7d404-2b0c-4378-f1a4-fd10be218c71"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95       198\n",
            "           1       0.98      0.89      0.93       155\n",
            "\n",
            "    accuracy                           0.94       353\n",
            "   macro avg       0.95      0.94      0.94       353\n",
            "weighted avg       0.95      0.94      0.94       353\n",
            "\n",
            "ROC-AUC Score: 0.99\n",
            "\n",
            "Business Impact:\n",
            "Implementing this email classification system can significantly improve efficiency by automatically filtering spam, reducing the time employees spend sorting emails.\n",
            "It can also enhance security by minimizing exposure to malicious content often found in spam.\n",
            "Accurate classification leads to a better user experience and ensures important communications are not missed.\n",
            "The chosen metrics (precision, recall, F1-score, ROC-AUC) are relevant for evaluating a spam filter. High precision minimizes legitimate emails being marked as spam, while high recall minimizes spam emails reaching the inbox. ROC-AUC provides an overall measure of the model's ability to distinguish between classes.\n"
          ]
        }
      ]
    }
  ]
}